# -*- coding: utf-8 -*-
"""Random forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a90GR-jerIslltYvqVQ9fE0J885Uq8k0
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()
df = pd.read_csv("customer_churn (3).csv")

df.info()
#we can see here total charges is in float but it is given as object,next, correct it to float

df['TotalCharges']=pd.to_numeric(df['TotalCharges'], errors='coerce')
#errors='coerce' means it will make it as null value non numerical dtype

#TotalCharges is now float
df.dtypes

"""Missing value analysis"""

#drop null values
df.dropna(inplace=True)
#drop customerId
df.drop(columns=['customerID'], inplace=True)

df.describe(include='object').T

"""Label encoding means-object type to integer like yes=1 and no=0"""

df['Partner']=df['Partner'].map({'Yes':1,'No':0})
df['Dependents']=df['Dependents'].map({'Yes':1,'No':0})
df['MultipleLines']=df['MultipleLines'].map({'No phone service':0,'Yes':2,'No':1})
df['InternetService']=df['InternetService'].map({'DSL':0,'Fiber optic':1,'No':2})
df['OnlineSecurity']=df['OnlineSecurity'].map({'Yes':1,'No':0,'No internet service':2})
df['OnlineBackup']=df['OnlineBackup'].map({'Yes':1,'No':0,'No internet service':2})
df['DeviceProtection']=df['DeviceProtection'].map({'Yes':1,'No':0,'No internet service':2})
df['TechSupport']=df['TechSupport'].map({'Yes':1,'No':0,'No internet service':2})
df['StreamingTV']=df['StreamingTV'].map({'Yes':1,'No':0,'No internet service':2})
df['StreamingMovies']=df['StreamingMovies'].map({'Yes':1,'No':0,'No internet service':2})
df['Contract']=df['Contract'].map({'Month-to-month':0,'One year':1,'Two year':2})
df['PaperlessBilling']=df['PaperlessBilling'].map({'Yes':1,'No':0})
df['PaymentMethod']=df['PaymentMethod'].map({'Electronic check':1,'Mailed check':0,'Bank transfer (automatic)':2,'Credit card (automatic)':3})
df['gender']=df['gender'].map({'Female':1,'Male':0})
df['PhoneService']=df['PhoneService'].map({'Yes':1,'No':0})

df.head()

"""DATA VISUALIZATION"""

df["Churn"].value_counts(normalize=True)

df["Churn"].value_counts(normalize=True).plot(kind='bar',figsize=(6,8),fontsize=13)

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15,12))
sns.heatmap(df.iloc[:,:-1].corr(),cmap="RdYlBu",annot=True,fmt=".1f")
#fmt=".1f" means it's decimal count,2f 2 decimal
plt.show

df['Churn']=df['Churn'].map({'Yes':1,'No':0})

"""Model building"""

x=df.iloc[:,:-1].values
y=df['Churn']. values
#.values makes an array without.values it is pd

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

clf=tree.DecisionTreeClassifier()
clf.fit(x_train,y_train)
y_train_pred=clf.predict(x_train)
y_test_pred=clf.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Train score {accuracy_score(y_test_pred,y_test)}')

#Now from random forest

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(random_state=45)
rf1=rf.fit(x_train,y_train)
y_train_pred=rf.predict(x_train)
y_test_pred=rf.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Train score {accuracy_score(y_test_pred,y_test)}')

"""Hyper parameter tuning"""

from sklearn.model_selection import RandomizedSearchCV
#number of trees in random forest
n_estimators=[int(x) for x in np.linspace(200,2000,10)]
#number of features to consider at every split
max_features=['auto','sqrt']
#maximum number of levels in tree
max_depth=[int(x) for x in np.linspace(10,110,11)]
max_depth.append(None)
#minimum number of samples required to split a node
min_samples_split=[2,5,10]
#minimum number of samples required at each leaf node
min_samples_leaf=[1,2,4]
#method of selecting samples for training each tree
bootstrap=[True,False]
random_grid={'n_estimators':n_estimators, 'max_features':max_features, 'max_depth':max_depth, 'min_samples_split':min_samples_split, 'min_samples_leaf':min_samples_leaf, 'bootstrap': bootstrap}
print(random_grid)

#use random grid for best hyper parameter
#first create the base model to tube
rf=RandomForestClassifier (random_state=42)
#Random search of paramet6, using 3 fold cross validation
#search accross 100 different combinations andd use  all available cores
rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,scoring='neg_mean_absolute_error',cv=3,verbose=2,random_state=42,n_jobs=-1,return_train_score=True)
#Fit the random search model
rf2=rf_random.fit(x_train,y_train);#use random grid for best hyper parameter
#first create the base model to tube
rf=RandomForestClassifier (random_state=42)
#Random search of paramet6, using 3 fold cross validation
#search accross 100 different combinations andd use  all available cores
rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,scoring='neg_mean_absolute_error',cv=3,verbose=2,random_state=42,n_jobs=-1,return_train_score=True)
#Fit the random search model
rf2=rf_random.fit(x_train,y_train);

rf2.best_params_

rf2=RandomForestClassifier (n_estimators=2000,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',max_depth=10, bootstrap=False)
rf2=rf2.fit(x_train,y_train)

y_train_pred=rf2.predict(x_train)
y_test_pred=rf2.predict(x_test)
print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Train score {accuracy_score(y_test_pred,y_test)}')

"""Model evaluation"""

importance=rf2.feature_importances_
importance=pd.Series(importance)
importance
#After training a RandomForestClassifier (or Regressor), the model can tell you which features were most important in making predictions.

#It calculates importance by measuring how much each feature reduces impurity (like Gini or entropy) across all trees.

df2=pd.DataFrame(columns=['Feature','Importance'])
cols=list(df.drop('Churn',axis=1). columns)
df2['Feature']=cols
df2['Importance']=importance
df2

df2.sort_values(by='Importance', ascending=False)

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(25,6))
sns.barplot(x='Feature',y="Importance",data=df2,order=df2.sort_values('Importance', ascending=False). Feature)

